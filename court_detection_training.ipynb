{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.1.45)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (2024.8.30)\n",
      "Requirement already satisfied: idna==3.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (1.4.5)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (1.26.2)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/anagireddygari/Library/Python/3.11/lib/python/site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in /Users/anagireddygari/Library/Python/3.11/lib/python/site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (2.2.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->roboflow) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->roboflow) (4.47.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anagireddygari/Library/Python/3.11/lib/python/site-packages (from matplotlib->roboflow) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->roboflow) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->roboflow) (3.3.2)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in squash_court_segmentation-1 to coco-segmentation:: 100%|██████████| 11532/11532 [00:00<00:00, 14881.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to squash_court_segmentation-1 in coco-segmentation:: 100%|██████████| 233/233 [00:00<00:00, 2441.94it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "from utils.config import ultralytics_key\n",
    "\n",
    "rf = Roboflow(api_key=ultralytics_key)\n",
    "project = rf.workspace(\"squashcourtkeypoints\").project(\"squash_court_segmentation\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"coco-segmentation\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pycocotools-mac (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pycocotools-mac\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install pycocotools-mac --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered annotations saved to /Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/test/_filtered_annotations.coco.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def filter_annotations(input_json_path, output_json_path):\n",
    "    # Load the annotations JSON file\n",
    "    with open(input_json_path, 'r') as f:\n",
    "        annotations_data = json.load(f)\n",
    "\n",
    "    # Extract images and annotations\n",
    "    images = annotations_data['images']\n",
    "    annotations = annotations_data['annotations']\n",
    "\n",
    "    # Create a set of image IDs with annotations\n",
    "    annotated_image_ids = {annotation['image_id'] for annotation in annotations}\n",
    "\n",
    "    # Filter out images with no annotations\n",
    "    filtered_images = [image for image in images if image['id'] in annotated_image_ids]\n",
    "\n",
    "    # Create new annotations data\n",
    "    filtered_annotations_data = {\n",
    "        'images': filtered_images,\n",
    "        'annotations': [annotation for annotation in annotations if annotation['image_id'] in annotated_image_ids],\n",
    "        'categories': annotations_data['categories']  # Retain categories if needed\n",
    "    }\n",
    "\n",
    "    # Save the filtered annotations to a new JSON file\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(filtered_annotations_data, f, indent=4)\n",
    "\n",
    "    print(f\"Filtered annotations saved to {output_json_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_json_path = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/test/_annotations.coco.json'  # Change this to your input file path\n",
    "output_json_path = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/test/_filtered_annotations.coco.json'  # Change this to your desired output file path\n",
    "\n",
    "filter_annotations(input_json_path, output_json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading COCO annotations\n",
    "class SquashCourtDataset(Dataset):\n",
    "    def __init__(self, root, annotation_file, transforms=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "\n",
    "        # Load image\n",
    "        img_info = self.coco.imgs[img_id]\n",
    "        img_path = os.path.join(self.root, img_info['file_name'])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        \n",
    "        \n",
    "        # Get target bounding boxes and segmentation masks\n",
    "        boxes = []\n",
    "        masks = []\n",
    "        labels = []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            masks.append(self.coco.annToMask(ann))\n",
    "            labels.append(ann['category_id'])  # Assuming \"1\" for squash_court\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': torch.tensor([img_id]),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "def get_transforms(train=True):\n",
    "    transforms_list = []\n",
    "    transforms_list.append(transforms.ToTensor())  # Convert PIL image to tensor\n",
    "    return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "def get_datasets(train_dir, train_ann, val_dir, val_ann, test_dir, test_ann):\n",
    "    train_dataset = SquashCourtDataset(\n",
    "        root=train_dir,\n",
    "        annotation_file=train_ann,\n",
    "        transforms=get_transforms(train=True)\n",
    "    )\n",
    "\n",
    "    val_dataset = SquashCourtDataset(\n",
    "        root=val_dir,\n",
    "        annotation_file=val_ann,\n",
    "        transforms=get_transforms(train=False)\n",
    "    )\n",
    "\n",
    "    test_dataset = SquashCourtDataset(\n",
    "        root=test_dir,\n",
    "        annotation_file=test_ann,\n",
    "        transforms=get_transforms(train=False)\n",
    "    )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Modify the paths to your dataset's directories and annotation files\n",
    "train_dir = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/train'\n",
    "train_ann = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/train/_annotations.coco.json'\n",
    "val_dir = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/valid'\n",
    "val_ann = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/valid/_annotations.coco.json'\n",
    "test_dir = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/test'\n",
    "test_ann = '/Users/anagireddygari/Desktop/Honors Project/Honors-Project-Player-Tracking-in-Squash-for-Analytics/training/squash_court_segmentation-1/test/_filtered_annotations.coco.json'\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(train_dir, train_ann, val_dir, val_ann, test_dir, test_ann)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=9, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained Mask R-CNN model and modify it for the number of classes\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained model on COCO\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Replace the head for fine-tuning\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace mask predictor for segmentation\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize model, optimizer, and learning rate scheduler\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "num_classes = 9  # Background and Squash Court\n",
    "model = get_model(num_classes)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer and learning rate scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=0.005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Training Loss: 293277063973270.0\n",
      "VAL LOSS:  {'loss_classifier': tensor(32382.7598), 'loss_box_reg': tensor(397044.8125), 'loss_mask': tensor(134078.5625), 'loss_objectness': tensor(5657.0503), 'loss_rpn_box_reg': tensor(13333.0127)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23281.5898), 'loss_box_reg': tensor(388053.0938), 'loss_mask': tensor(112906.3672), 'loss_objectness': tensor(1314.3478), 'loss_rpn_box_reg': tensor(9800.4395)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(33094.4688), 'loss_box_reg': tensor(392334.3125), 'loss_mask': tensor(139238.5469), 'loss_objectness': tensor(1780.7426), 'loss_rpn_box_reg': tensor(13350.1455)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(22772.6367), 'loss_box_reg': tensor(385014.8125), 'loss_mask': tensor(113182.5234), 'loss_objectness': tensor(4819.0088), 'loss_rpn_box_reg': tensor(9432.9814)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23795.5879), 'loss_box_reg': tensor(393571.4375), 'loss_mask': tensor(115216.5703), 'loss_objectness': tensor(1535.1899), 'loss_rpn_box_reg': tensor(9987.5723)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(32092.9004), 'loss_box_reg': tensor(387098.9375), 'loss_mask': tensor(138066.1094), 'loss_objectness': tensor(5440.2280), 'loss_rpn_box_reg': tensor(12870.8174)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(24970.4062), 'loss_box_reg': tensor(474976.6250), 'loss_mask': tensor(136606.4688), 'loss_objectness': tensor(4972.6812), 'loss_rpn_box_reg': tensor(13868.9131)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23982.9219), 'loss_box_reg': tensor(396285.5312), 'loss_mask': tensor(116068.7422), 'loss_objectness': tensor(1293.0889), 'loss_rpn_box_reg': tensor(10033.7217)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(25625.6387), 'loss_box_reg': tensor(468686.), 'loss_mask': tensor(147532.1562), 'loss_objectness': tensor(3273.8015), 'loss_rpn_box_reg': tensor(12191.7441)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(33154.5664), 'loss_box_reg': tensor(399005.7812), 'loss_mask': tensor(135297.7031), 'loss_objectness': tensor(3768.6436), 'loss_rpn_box_reg': tensor(13261.2002)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23796.9453), 'loss_box_reg': tensor(393738.6875), 'loss_mask': tensor(115442.8281), 'loss_objectness': tensor(2039.1553), 'loss_rpn_box_reg': tensor(9924.5576)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23585.0352), 'loss_box_reg': tensor(389127.8438), 'loss_mask': tensor(113843.0391), 'loss_objectness': tensor(2538.6316), 'loss_rpn_box_reg': tensor(9861.2549)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23637.7969), 'loss_box_reg': tensor(389524.4062), 'loss_mask': tensor(114167.7891), 'loss_objectness': tensor(2367.2949), 'loss_rpn_box_reg': tensor(9883.0947)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(33394.3750), 'loss_box_reg': tensor(395514.0312), 'loss_mask': tensor(140139.7656), 'loss_objectness': tensor(2358.6465), 'loss_rpn_box_reg': tensor(13465.4727)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(32492.1016), 'loss_box_reg': tensor(389320.7500), 'loss_mask': tensor(137570.0625), 'loss_objectness': tensor(1623.8544), 'loss_rpn_box_reg': tensor(13264.9316)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23732.7539), 'loss_box_reg': tensor(391996.8750), 'loss_mask': tensor(114775.0078), 'loss_objectness': tensor(1647.4351), 'loss_rpn_box_reg': tensor(9880.8203)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23798.2070), 'loss_box_reg': tensor(390704.6250), 'loss_mask': tensor(114697.6641), 'loss_objectness': tensor(1671.5228), 'loss_rpn_box_reg': tensor(9927.3047)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(33305.2734), 'loss_box_reg': tensor(394658.1562), 'loss_mask': tensor(139705.5469), 'loss_objectness': tensor(1904.4589), 'loss_rpn_box_reg': tensor(13463.0518)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(25347.6875), 'loss_box_reg': tensor(476021.3750), 'loss_mask': tensor(142939.3906), 'loss_objectness': tensor(1631.1809), 'loss_rpn_box_reg': tensor(12551.1953)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(23733.1055), 'loss_box_reg': tensor(392019.8125), 'loss_mask': tensor(114855.6328), 'loss_objectness': tensor(2071.9033), 'loss_rpn_box_reg': tensor(9911.0430)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(14249.7871), 'loss_box_reg': tensor(382926.8750), 'loss_mask': tensor(94012.8594), 'loss_objectness': tensor(1037.2229), 'loss_rpn_box_reg': tensor(6015.3369)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(26360.0410), 'loss_box_reg': tensor(493347.4375), 'loss_mask': tensor(147417.5781), 'loss_objectness': tensor(1843.6038), 'loss_rpn_box_reg': tensor(12948.3770)}\n",
      "VAL LOSS:  {'loss_classifier': tensor(32014.3047), 'loss_box_reg': tensor(391501.1875), 'loss_mask': tensor(131955.4844), 'loss_objectness': tensor(872.9545), 'loss_rpn_box_reg': tensor(13557.0781)}\n",
      "Validation Loss: 573784.2364130435\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # To accumulate the losses over all batches\n",
    "\n",
    "    for images, targets in train_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += losses.item()\n",
    "\n",
    "    # Step the learning rate scheduler after every epoch\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            model.train()\n",
    "            # Forward pass for validation\n",
    "            val_loss_dict = model(images, targets)\n",
    "            print(\"VAL LOSS: \", val_loss_dict)\n",
    "            val_loss = sum(loss for loss in val_loss_dict.values())\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            running_val_loss += val_loss.item()\n",
    "            model.eval()\n",
    "\n",
    "    # Print average validation loss for the epoch\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_loader:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        outputs = model(images, targets)\n",
    "        print(f\"Test outputs: {outputs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.stat_dict(), 'keypoints_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
